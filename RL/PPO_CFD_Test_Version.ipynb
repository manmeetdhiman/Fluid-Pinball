{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from scipy import interpolate\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment_Parabola:\n",
    "    def __init__(self, x=90.0,y=-90.0,z=90.0):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.z=z\n",
    "        \n",
    "        self.x_reset=x\n",
    "        self.y_reset=y\n",
    "        self.z_reset=z\n",
    "        \n",
    "        self.f=self.calculate_f()\n",
    "        self.counter=0\n",
    "        self.done=False\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.x=self.x_reset\n",
    "        self.y=self.y_reset\n",
    "        self.z=self.z_reset\n",
    "        \n",
    "        self.f=self.calculate_f()\n",
    "        self.counter=0\n",
    "        self.done=False\n",
    "        observation=np.array([self.x/100.0,self.y/100.0,self.z/100.0, 0.0])\n",
    "        \n",
    "        return(observation)\n",
    "    \n",
    "    def step (self,action):\n",
    "        action_x=action[0]*100.0\n",
    "        action_y=action[1]*100.0\n",
    "        action_z=action[2]*100.0\n",
    "        \n",
    "        self.x=action_x\n",
    "        self.y=action_y\n",
    "        self.z=action_z\n",
    "                \n",
    "        f_=self.calculate_f()\n",
    "        #reward=np.float64(-f_/1000*np.exp(0.2*(self.counter-20)))\n",
    "        reward=np.float64(-f_/4000*self.counter**0.5)\n",
    "        self.f=f_\n",
    "        self.counter +=1\n",
    "        observation=np.array([self.x/100.0,self.y/100.0,self.z/100.0, self.counter/100.0])\n",
    "        \n",
    "        return observation,reward\n",
    "    \n",
    "    def calculate_f(self):\n",
    "        f=(self.x-10.0)**2.0+(self.y)**2.0+(self.z+50.0)**2.0\n",
    "        return f\n",
    "    \n",
    "    def calculate_final_f(self,observation):\n",
    "        x=observation[0]\n",
    "        y=observation[1]\n",
    "        z=observation[2]\n",
    "        f=(x-10.0)**2+(y)**2+(z+50)**2\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment_Rastingin:\n",
    "    def __init__(self, x=5.0,y=5.0,z=-5.0):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.z=z\n",
    "        \n",
    "        self.x_reset=x\n",
    "        self.y_reset=y\n",
    "        self.z_reset=z\n",
    "        \n",
    "        self.f=self.calculate_f()\n",
    "        self.counter=0.0\n",
    "        self.done=False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x=self.x_reset\n",
    "        self.y=self.y_reset\n",
    "        self.z=self.z_reset\n",
    "        \n",
    "        self.f=self.calculate_f()\n",
    "        self.counter=0.0\n",
    "        self.done=False\n",
    "        observation=np.array([self.x/5.0,self.y/5.0,self.z/5.0, self.counter/100.0])\n",
    "        return(observation)\n",
    "    \n",
    "    def step (self,action):\n",
    "        \n",
    "        action_x=action[0]\n",
    "        action_y=action[1]\n",
    "        action_z=action[2]\n",
    "        \n",
    "        self.x+=action_x\n",
    "        self.y+=action_y\n",
    "        self.z+=action_z\n",
    "        \n",
    "        if self.x>=5.12:\n",
    "            self.x=5.12\n",
    "        if self.x<=-5.12:\n",
    "            self.x=-5.12\n",
    "        if self.y>=5.12:\n",
    "            self.y=5.12\n",
    "        if self.y<=-5.12:\n",
    "            self.y=-5.12\n",
    "        if self.z>=5.12:\n",
    "            self.z=5.12\n",
    "        if self.z<=-5.12:\n",
    "            self.z=-5.12\n",
    "                \n",
    "        f_=self.calculate_f()\n",
    "        reward=np.float64(-f_/25.0*np.exp(0.2*(self.counter-20)))\n",
    "        self.f=f_\n",
    "        self.counter +=1\n",
    "        observation=np.array([self.x/5.0,self.y/5.0,self.z/5.0, self.counter/100.0])\n",
    "        \n",
    "        return observation,reward\n",
    "    \n",
    "    def calculate_f(self):\n",
    "        A=10\n",
    "        d=3\n",
    "        T1=self.x**2-A*np.cos(2*3.14*self.x)\n",
    "        T2=self.y**2-A*np.cos(2*3.14*self.y)\n",
    "        T3=self.z**2-A*np.cos(2*3.14*self.z)\n",
    "        f=A*d+T1+T2+T3\n",
    "        return f\n",
    "    \n",
    "    def calculate_final_f(self,state):\n",
    "        x=state[0]*5\n",
    "        y=state[1]*5\n",
    "        z=state[2]*5\n",
    "        A=10\n",
    "        d=3\n",
    "        T1=x**2-A*np.cos(2*3.14*x)\n",
    "        T2=y**2-A*np.cos(2*3.14*y)\n",
    "        T3=z**2-A*np.cos(2*3.14*z)\n",
    "        f=np.array([A*d+T1+T2+T3])\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actor network class created below\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int,):\n",
    "        \n",
    "        self.in_dim=in_dim\n",
    "        self.out_dim=out_dim\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.hidden_one = nn.Linear(in_dim, 320)\n",
    "        self.hidden_two = nn.Linear(320,320)\n",
    "        \n",
    "        self.mu_layer = nn.Linear(320, out_dim)\n",
    "        self.log_std_layer = nn.Linear(320, out_dim)\n",
    "\n",
    "    def forward(self, state: torch.Tensor):\n",
    "        \n",
    "        x = F.tanh(self.hidden_one(state))\n",
    "        x = F.tanh(self.hidden_two(x))\n",
    "        \n",
    "        mu = torch.tanh(self.mu_layer(x))\n",
    "        log_std = torch.tanh(self.log_std_layer(x))\n",
    "\n",
    "        std = torch.exp(log_std)\n",
    "        #std = torch.ones((1,self.out_dim), dtype=torch.float64)\n",
    "        #std = std.new_full((1, self.out_dim), 1)\n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.sample()\n",
    "\n",
    "        return action, dist, mu, std\n",
    "    \n",
    "#Critic network class created below    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.hidden_one = nn.Linear(in_dim, 320)\n",
    "        self.hidden_two = nn.Linear(320,320)\n",
    "        self.out = nn.Linear(320, 1)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        x = F.relu(self.hidden_one(state))\n",
    "        x = F.relu(self.hidden_two(x))\n",
    "        value = self.out(x)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_Agent(object):\n",
    "    def __init__(self, obs_dim=3, act_dim =3, gamma =0.99,lamda =0.9,\n",
    "                 entropy_coef =0.01,epsilon =0.2, value_range =0.2,\n",
    "                 num_epochs =10, batch_size =30, actor_lr =0.001, critic_lr =0.001):\n",
    "        \n",
    "        self.gamma=gamma\n",
    "        self.lamda=lamda\n",
    "        self.entropy_coef=entropy_coef\n",
    "        self.epsilon=epsilon\n",
    "        self.value_range=value_range\n",
    "       \n",
    "        self.num_epochs=num_epochs\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.obs_dim=obs_dim\n",
    "        self.act_dim=act_dim\n",
    "        self.actor_lr=actor_lr\n",
    "        self.critic_lr=critic_lr\n",
    "        \n",
    "        self.actor=Actor(self.obs_dim,self.act_dim)\n",
    "        self.critic=Critic(self.obs_dim)\n",
    "        self.actor_optimizer=optim.Adam(self.actor.parameters(),lr=self.actor_lr)\n",
    "        self.critic_optimizer=optim.Adam(self.critic.parameters(),lr=self.critic_lr)\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.next_states=[]\n",
    "        \n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    def clear_memory(self):\n",
    "    \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.next_states= []\n",
    "        \n",
    "    def get_gae(self, rewards: list, values: list, is_terminals: list, gamma: float, lamda: float,):\n",
    "        \n",
    "        gae = 0\n",
    "        returns = []\n",
    "    \n",
    "        for i in reversed(range(len(rewards))):\n",
    "            delta = (rewards[i] + gamma * values[i + 1] * is_terminals[i] - values[i])\n",
    "            gae = delta + gamma * lamda * is_terminals[i] * gae\n",
    "            return_=np.array([gae+values[i]])\n",
    "            returns.insert(0, return_)\n",
    "\n",
    "        return returns\n",
    "    \n",
    "    def trajectories_data_generator(self, states: torch.Tensor,actions: torch.Tensor,\n",
    "                                returns: torch.Tensor,log_probs: torch.Tensor,\n",
    "                                values: torch.Tensor,advantages: torch.Tensor,\n",
    "                                batch_size, num_epochs,):\n",
    "        data_len = states.size(0)\n",
    "        for _ in range(num_epochs):\n",
    "            for _ in range(data_len // batch_size):\n",
    "                ids = np.random.choice(data_len, batch_size)\n",
    "                yield states[ids, :], actions[ids], returns[ids], log_probs[ids], values[ids], advantages[ids]\n",
    "\n",
    "        \n",
    "    def _get_action(self,state):\n",
    "        \n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action,dist,mu,std=self.actor.forward(state)   \n",
    "        value=self.critic.forward(state)\n",
    "        \n",
    "        return action, value, dist, mu, std\n",
    "    \n",
    "    \n",
    "    def _update_weights(self):\n",
    "        \n",
    "        self.rewards=torch.tensor(self.rewards).float()\n",
    "        self.is_terminals=torch.tensor(self.is_terminals).float()\n",
    "        self.values=torch.tensor(self.values).float()\n",
    "        self.states=torch.tensor(self.states).float()\n",
    "        self.log_probs=torch.tensor(self.log_probs).float()\n",
    "        self.actions=torch.tensor(self.actions).float()\n",
    "        \n",
    "        returns = self.get_gae(self.rewards, self.values,self.is_terminals,\n",
    "                          self.gamma,self.lamda,)\n",
    "        returns=torch.tensor(returns).float()\n",
    "\n",
    "        states=self.states\n",
    "        actions=self.actions\n",
    "        log_probs=self.log_probs\n",
    "        values=self.values\n",
    "        advantages = returns - values[:-1]\n",
    "\n",
    "        for state, action, return_, old_log_prob, old_value, advantage in self.trajectories_data_generator(\n",
    "            states=states,actions=actions,returns=returns,log_probs=log_probs,values=values,\n",
    "            advantages=advantages, batch_size=self.batch_size,num_epochs=self.num_epochs,):\n",
    "\n",
    "            # compute ratio (pi_theta / pi_theta__old)\n",
    "            _, dist,__,___ = self.actor(state)\n",
    "            cur_log_prob = dist.log_prob(action)\n",
    "            ratio = torch.exp(cur_log_prob - old_log_prob)\n",
    "\n",
    "            # compute entropy\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            # compute actor loss\n",
    "            loss =  advantage * ratio\n",
    "            clipped_loss = (torch.clamp(ratio, 1. - self.epsilon, 1. + self.epsilon)\n",
    "                            * advantage)\n",
    "            actor_loss = (-torch.mean(torch.min(loss, clipped_loss))\n",
    "                          - entropy * self.entropy_coef)\n",
    "            \n",
    "            # critic loss, uncoment for clipped value loss too.\n",
    "            cur_value = self.critic(state)\n",
    "            clipped_value = (\n",
    "                old_value + torch.clamp(cur_value - old_value,\n",
    "                                        -self.value_range, self.value_range)\n",
    "               )\n",
    "            loss = (return_ - cur_value).pow(2)\n",
    "            clipped_loss = (return_ - clipped_value).pow(2)\n",
    "            critic_loss = torch.mean(torch.max(loss, clipped_loss))\n",
    "\n",
    "            critic_loss = (return_ - cur_value).pow(2).mean()\n",
    "\n",
    "            # actor optimizer step\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # critic optimizer step\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "            \n",
    "        self.clear_memory()\n",
    "        \n",
    "    def _save_weights(self,policy_num: int):\n",
    "            \n",
    "        filename_actor=\"Actor_Policy_Number_\"+str(policy_num)\n",
    "        filename_critic=\"Critic_Policy_Number_\"+str(policy_num)\n",
    "        torch.save(self.actor.state_dict(),filename_actor)\n",
    "        torch.save(self.critic.state_dict(),filename_critic)\n",
    "        \n",
    "    def _load_weights(self,filename_actor,filename_critic):\n",
    "            \n",
    "        self.actor.load_state_dict(torch.load(filename_actor))\n",
    "        self.actor.eval()\n",
    "        self.critic.load_state_dict(torch.load(filename_critic))\n",
    "        self.actor.eval()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PI_Motor(w_des_two,tsteps,dt,w0): \n",
    "    \n",
    "    w_des_one=np.zeros(50)\n",
    "    for i in range(len(w_des_one)):\n",
    "        if i<=20:\n",
    "            w_des_one[i]=0\n",
    "        else:\n",
    "            w_des_one[i]=w0\n",
    "\n",
    "    w_des=np.zeros(len(w_des_one)+len(w_des_two))\n",
    "    for i in range(len(w_des)):\n",
    "        if i<len(w_des_one):\n",
    "            w_des[i]=w_des_one[i]\n",
    "        else:\n",
    "            w_des[i]=w_des_two[i-len(w_des_one)]\n",
    "    \n",
    "    tsteps=tsteps+len(w_des_one)\n",
    "    \n",
    "    #Simulation\n",
    "    # Motor Parameters\n",
    "    Ke = 0.021199438  # V/rad/s\n",
    "    Kt = 0.0141937  # Nm/A\n",
    "    b = 0.0001011492  # Nm/rad/s\n",
    "    L = 0.00075  # H\n",
    "    J = 0.00000109445  # kgm^2\n",
    "    R = 1.56  # ohms\n",
    "    V_max = 36\n",
    "    V_min = -V_max\n",
    "    \n",
    "    # simulation time parameters\n",
    "    tf = dt * (tsteps - 1)\n",
    "    t_sim = np.linspace(0, tf, tsteps)\n",
    "    \n",
    "    #PID parameters\n",
    "    error_s = np.zeros(tsteps)\n",
    "    V_bias = 0\n",
    "    tau_i = 30\n",
    "    sum_int = 0.0\n",
    "    \n",
    "    #Initial Conditions\n",
    "    i0=0\n",
    "    w0=0\n",
    "    \n",
    "    #Tunable parameters\n",
    "    Kp = 0.270727147578817\n",
    "    Ki = 50.0897752327866\n",
    "    Kd = 0.000141076220179068\n",
    "    N = 248711.202620588 #Filter coefficient for filtered derivative\n",
    "    \n",
    "    # Motor input\n",
    "    V_in = np.zeros(tsteps)\n",
    "    \n",
    "    # ODE Output Storage\n",
    "    w_s = np.zeros(tsteps)\n",
    "    i_s = np.zeros(tsteps)\n",
    "    \n",
    "    # DC Motor Model ODEs\n",
    "    def motor_electrical(i, t, V, w):\n",
    "        di_dt = (V - R * i - Ke * w) / L\n",
    "        return di_dt\n",
    "    \n",
    "    def motor_mechanical(w, t, i):\n",
    "        dw_dt = (Kt * i - b * w) / J\n",
    "        return dw_dt\n",
    "    \n",
    "    # sim_loop\n",
    "    for n in range(tsteps - 1):\n",
    "        # PID Control\n",
    "        error = w_des[n + 1] - w0\n",
    "        error_s[n + 1] = error\n",
    "        \n",
    "        sum_int = sum_int + error * dt\n",
    "        de_dt = (error_s[n+1] - error_s[n])/dt\n",
    "        V_PID = V_bias + Kp * error + Ki * sum_int + (N*Kd)/(1 + N*sum_int) + Kd*de_dt\n",
    "        \n",
    "        # anti-integral windup\n",
    "        \n",
    "        if V_PID > V_max:\n",
    "            V_PID = V_max\n",
    "            sum_int = sum_int - error * dt\n",
    "            \n",
    "        if V_PID < V_min:\n",
    "            V_PID = V_min\n",
    "            sum_int = sum_int - error * dt\n",
    "            \n",
    "        # PID Data storage\n",
    "        int_s = sum_int\n",
    "        V_in[n] = V_PID\n",
    "        # Motor Actuation\n",
    "        V = V_in[n]\n",
    "        t_range = [t_sim[n], t_sim[n + 1]]\n",
    "\n",
    "        i = odeint(motor_electrical, i0, t_range, args=(V, w0))\n",
    "        i0 = i[-1][0]\n",
    "        i_s[n + 1] = i0\n",
    "        w = odeint(motor_mechanical, w0, t_range, args=(i0,))\n",
    "        w0 = w[-1]\n",
    "        w_s[n + 1] = w0\n",
    "    \n",
    "    w_final=w_s[len(w_des_one):]\n",
    "    return w_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Spline(times,rotations,des_times,k=3):\n",
    "    spline = interpolate.splrep(times,rotations)\n",
    "    des_rotations=interpolate.splev(des_times,spline)\n",
    "    return des_rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CFD_Run(iteration_ID,action_counter,time_step_start,time_step_end,mot_data):\n",
    "    \n",
    "    front_cyl=mot_data['revolutions'][0]\n",
    "    top_cyl=mot_data['revolutions'][1]\n",
    "    bot_cyl=mot_data['revolutions'][2]\n",
    "    \n",
    "    top_sens=np.zeros(len(front_cyl))\n",
    "    mid_sens=np.zeros(len(front_cyl))\n",
    "    bot_sens=np.zeros(len(front_cyl))\n",
    "    \n",
    "    for i in range(len(front_cyl)):\n",
    "        top_sens[i]=front_cyl[i]/500\n",
    "        mid_sens[i]=top_cyl[i]/500\n",
    "        bot_sens[i]=bot_cyl[i]/500\n",
    "    \n",
    "    vel_data={'top':top_sens,'mid':mid_sens,'bot':bot_sens}\n",
    "    return vel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Iteration():\n",
    "    def __init__(self, iteration_ID=1, shedding_freq=8.64, num_actions=25,dur_actions=0.2,\n",
    "                 CFD_timestep=5e-4,mot_timestep=8e-4,dur_ramps=0.04):\n",
    "        \n",
    "        self.iteration_ID=iteration_ID\n",
    "        self.shedding_freq=shedding_freq\n",
    "        self.num_actions=num_actions\n",
    "        self.dur_actions=dur_actions\n",
    "        self.CFD_timestep=CFD_timestep\n",
    "        self.mot_timestep=mot_timestep\n",
    "        self.dur_ramps= dur_ramps\n",
    "        \n",
    "        self.states=[]\n",
    "        self.actions=[]\n",
    "        self.values=[]\n",
    "        self.mus=[]\n",
    "        self.stds=[]\n",
    "        self.rewards=[]\n",
    "        self.is_terminals=[]\n",
    "        self.log_probs=[]\n",
    "        self.next_states=[]\n",
    "        \n",
    "        self.front_cyl_RPS=[]\n",
    "        self.top_cyl_RPS=[]\n",
    "        self.bot_cyl_RPS=[]\n",
    "        \n",
    "        self.front_cyl_RPS_PI=[]\n",
    "        self.top_cyl_RPS_PI=[]\n",
    "        self.bot_cyl_RPS_PI=[]\n",
    "        \n",
    "        self.top_sens_values=[]\n",
    "        self.mid_sens_values=[]\n",
    "        self.bot_sens_values=[]\n",
    "        \n",
    "        self.action_counter=0\n",
    "        self.time_step_start=1\n",
    "        \n",
    "        self.shedding_period=1/self.shedding_freq\n",
    "        self.mot_timesteps_period=int(np.ceil(self.shedding_period/self.mot_timestep))\n",
    "        self.mot_timesteps_action=int(np.ceil(self.mot_timesteps_period*self.dur_actions))\n",
    "        self.mot_timesteps_ramp=int(np.ceil(self.mot_timesteps_period*self.dur_ramps))\n",
    "        self.CFD_timesteps_period=int(np.ceil(self.shedding_period/self.CFD_timestep))\n",
    "        self.CFD_timesteps_action=int(np.ceil(self.CFD_timesteps_period*self.dur_actions))\n",
    "        self.CFD_timesteps_ramp=int(np.ceil(self.CFD_timesteps_period*self.dur_ramps))\n",
    "    \n",
    "    def reset_state(self):\n",
    "        state=np.array([0.5,0.5,0.5,0.5])\n",
    "        return state \n",
    "    \n",
    "    def calculate_reward(self):\n",
    "        reward=np.array([1.7])\n",
    "        return reward\n",
    "    \n",
    "    def calculate_state(self):\n",
    "        state=np.array([0.6,0.6,0.6,0.6])\n",
    "        return state\n",
    "    \n",
    "    def convert_timestep_array(self,times_A,array_A,timestep_A):\n",
    "        if timestep_A==self.mot_timestep:\n",
    "            timestep_B=self.CFD_timestep\n",
    "            times_B=np.zeros(self.CFD_timesteps_action)\n",
    "            array_B=np.zeros(len(times_B))\n",
    "        else:\n",
    "            timestep_B=self.mot_timestep\n",
    "            times_B=np.zeros(self.mot_timesteps_action)\n",
    "            array_B=np.zeros(len(times_B))\n",
    "            \n",
    "        for i in range(len(times_B)):\n",
    "            times_B[i]=i*timestep_B\n",
    "            for j in range(len(times_A)):\n",
    "                if times_A[j]>=times_B[i]:\n",
    "                    array_B[i]=array_A[j]\n",
    "                    break\n",
    "                else:\n",
    "                    if i>0:\n",
    "                        array_B[i]=array_B[i-1]\n",
    "                \n",
    "        return array_B\n",
    "            \n",
    "    def calculate_mot_data(self,action):\n",
    "        front_cyl_RPS_new=action[0]*500\n",
    "        top_cyl_RPS_new=action[1]*500\n",
    "        bot_cyl_RPS_new=action[2]*500\n",
    "        \n",
    "        if front_cyl_RPS_new>1045:\n",
    "            front_cyl_RPS_new=1045\n",
    "        if front_cyl_RPS_new<-1045:\n",
    "            front_cyl_RPS_new=-1045\n",
    "        if top_cyl_RPS_new>1045:\n",
    "            top_cyl_RPS_new=1045\n",
    "        if top_cyl_RPS_new<-1045:\n",
    "            top_cyl_RPS_new=-1045\n",
    "        if bot_cyl_RPS_new>1045:\n",
    "            bot_cyl_RPS_new=1045\n",
    "        if bot_cyl_RPS_new<-1045:\n",
    "            bot_cyl_RPS_new=-1045\n",
    "        \n",
    "        des_times=np.zeros(self.mot_timesteps_action)\n",
    "        \n",
    "        for i in range(len(des_times)):\n",
    "            des_times[i]=self.mot_timestep*i\n",
    "        \n",
    "        times=np.zeros(10+self.mot_timesteps_action)\n",
    "        front_cyl_RPS_temp_mot=np.zeros(len(times))\n",
    "        top_cyl_RPS_temp_mot=np.zeros(len(times))\n",
    "        bot_cyl_RPS_temp_mot=np.zeros(len(times))\n",
    "        \n",
    "        for i in range(len(times)):\n",
    "            if i <5 and len(self.front_cyl_RPS_PI)>5:\n",
    "                times[i]=self.mot_timestep*(-5+i)\n",
    "                front_cyl_RPS_temp_mot[i]=self.front_cyl_RPS_PI[-5+i]\n",
    "                top_cyl_RPS_temp_mot[i]=self.top_cyl_RPS_PI[-5+i]\n",
    "                bot_cyl_RPS_temp_mot[i]=self.bot_cyl_RPS_PI[-5+i]\n",
    "            elif i<5 and len(self.front_cyl_RPS_PI)<=5:\n",
    "                times[i]=self.mot_timestep*(-5+i)\n",
    "                front_cyl_RPS_temp_mot[i]=0\n",
    "                top_cyl_RPS_temp_mot[i]=0\n",
    "                bot_cyl_RPS_temp_mot[i]=0\n",
    "            elif i >=5:\n",
    "                times[i]=self.mot_timestep*self.mot_timesteps_ramp+(i-5)*self.mot_timestep\n",
    "                front_cyl_RPS_temp_mot[i]=front_cyl_RPS_new\n",
    "                top_cyl_RPS_temp_mot[i]=top_cyl_RPS_new\n",
    "                bot_cyl_RPS_temp_mot[i]=bot_cyl_RPS_new\n",
    "        \n",
    "        front_cyl_RPS_temp_mot=Spline(times,front_cyl_RPS_temp_mot,des_times)\n",
    "        top_cyl_RPS_temp_mot=Spline(times,top_cyl_RPS_temp_mot,des_times)\n",
    "        bot_cyl_RPS_temp_mot=Spline(times,bot_cyl_RPS_temp_mot,des_times)\n",
    "        \n",
    "        front_cyl_RPS_temp_CFD=self.convert_timestep_array(des_times,front_cyl_RPS_temp_mot,self.mot_timestep)\n",
    "        top_cyl_RPS_temp_CFD=self.convert_timestep_array(des_times,top_cyl_RPS_temp_mot,self.mot_timestep)\n",
    "        bot_cyl_RPS_temp_CFD=self.convert_timestep_array(des_times,bot_cyl_RPS_temp_mot,self.mot_timestep)\n",
    "        \n",
    "        self.front_cyl_RPS.extend(front_cyl_RPS_temp_CFD)\n",
    "        self.top_cyl_RPS.extend(top_cyl_RPS_temp_CFD)\n",
    "        self.bot_cyl_RPS.extend(bot_cyl_RPS_temp_CFD)\n",
    "        \n",
    "        if len(self.front_cyl_RPS_PI)==0:\n",
    "            front_w0=0\n",
    "            top_w0=0\n",
    "            bot_w0=0\n",
    "        else:\n",
    "            front_w0=self.front_cyl_RPS_PI[-1]\n",
    "            top_w0=self.front_cyl_RPS_PI[-1]\n",
    "            bot_w0=self.bot_cyl_RPS_PI[-1]\n",
    "        \n",
    "        front_cyl_RPS_temp_mot=PI_Motor(front_cyl_RPS_temp_mot,self.mot_timesteps_action,self.mot_timestep,\n",
    "                                             front_w0)           \n",
    "        \n",
    "        top_cyl_RPS_temp_mot=PI_Motor(top_cyl_RPS_temp_mot,self.mot_timesteps_action,self.mot_timestep,\n",
    "                                             top_w0) \n",
    "        \n",
    "        bot_cyl_RPS_temp_mot=PI_Motor(bot_cyl_RPS_temp_mot,self.mot_timesteps_action,self.mot_timestep,\n",
    "                                             bot_w0)\n",
    "        \n",
    "        front_cyl_RPS_temp_CFD=self.convert_timestep_array(des_times,front_cyl_RPS_temp_mot,self.mot_timestep)\n",
    "        top_cyl_RPS_temp_CFD=self.convert_timestep_array(des_times,top_cyl_RPS_temp_mot,self.mot_timestep)\n",
    "        bot_cyl_RPS_temp_CFD=self.convert_timestep_array(des_times,bot_cyl_RPS_temp_mot,self.mot_timestep)\n",
    "        \n",
    "        self.front_cyl_RPS_PI.extend(front_cyl_RPS_temp_CFD)\n",
    "        self.top_cyl_RPS_PI.extend(top_cyl_RPS_temp_CFD)\n",
    "        self.bot_cyl_RPS_PI.extend(bot_cyl_RPS_temp_CFD)\n",
    "        \n",
    "        mot_data={'revolutions':[front_cyl_RPS_temp_CFD,top_cyl_RPS_temp_CFD,bot_cyl_RPS_temp_CFD]}        \n",
    "        \n",
    "        return mot_data\n",
    "    \n",
    "    def run_iteration(self):\n",
    "        #state=self.reset_state()\n",
    "        state=env.reset()\n",
    "        for i in range(num_actions):\n",
    "            self.action_counter += 1\n",
    "            \n",
    "            action,value,dist, mu,std=ppo_agent._get_action(state)\n",
    "            \n",
    "            log_prob=dist.log_prob(action)\n",
    "            log_prob=log_prob.detach().numpy()\n",
    "            action=np.array(action)\n",
    "            value = value.detach().numpy()\n",
    "            mu=mu.detach().numpy()\n",
    "            std=std.detach().numpy()\n",
    "            \n",
    "            self.states.append(state)\n",
    "            self.actions.append(action)\n",
    "            self.log_probs.append(log_prob)\n",
    "            self.values.append(value)\n",
    "            self.mus.append(mu)\n",
    "            self.stds.append(std)\n",
    "            \n",
    "            #mot_data=self.calculate_mot_data(action)\n",
    "            #time_step_end=self.time_step_start + self.CFD_timesteps_action\n",
    "            \n",
    "            #vel_data=CFD_Run(self.iteration_ID, self.action_counter,self.time_step_start,time_step_end, mot_data)\n",
    "            \n",
    "            #self.top_sens_values.extend(vel_data['top'])\n",
    "            #self.mid_sens_values.extend(vel_data['mid'])\n",
    "            #self.bot_sens_values.extend(vel_data['bot'])\n",
    "            \n",
    "            #reward=self.calculate_reward()\n",
    "            #state=self.calculate_state()\n",
    "            \n",
    "            state,reward=env.step(action)\n",
    "            \n",
    "            self.rewards.append(reward)\n",
    "            self.next_states.append(state)\n",
    "            if self.action_counter<self.num_actions:\n",
    "                is_terminal=np.array([1.0])\n",
    "            else:\n",
    "                is_terminal=np.array([0.0])\n",
    "            \n",
    "            self.is_terminals.append(is_terminal)\n",
    "            \n",
    "            #self.time_step_start = time_step_end + 1\n",
    "            \n",
    "    def save_iteration(self):\n",
    "        iteration_results={'iteration_ID':self.iteration_ID, 'states': self.states, 'actions': self.actions, \n",
    "                          'rewards':self.rewards, 'mus':self.mus,'stds':self.stds,'is_terminals':self.is_terminals,\n",
    "                          'log_probs':self.log_probs,'front_cyl_RPS':self.front_cyl_RPS,'top_cyl_RPS':self.top_cyl_RPS,\n",
    "                          'bot_cyl_RPS':self.bot_cyl_RPS,'front_cyl_RPS_PI':self.front_cyl_RPS_PI,\n",
    "                          'top_cyl_RPS_PI':self.top_cyl_RPS_PI,'bot_cyl_RPS_PI':self.bot_cyl_RPS_PI,\n",
    "                          'top_sens_values':self.top_sens_values,'mid_sens_values':self.mid_sens_values,\n",
    "                          'bot_sens_values':self.bot_sens_values}\n",
    "        \n",
    "        filename='data_iteration_'+str(self.iteration_ID)+'.pickle'\n",
    "        with open(filename, 'wb') as handle:\n",
    "            pickle.dump(iteration_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshat\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\nn\\functional.py:1614: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number:  1  Iteration Reward:  -11.209452663211515 Final F Value:  [68.37339494]\n",
      "Iteration Number:  2  Iteration Reward:  -13.83226702758111 Final F Value:  [94.83321568]\n",
      "Iteration Number:  3  Iteration Reward:  -9.860049483997729 Final F Value:  [41.73159556]\n",
      "Iteration Number:  4  Iteration Reward:  -11.855167848668716 Final F Value:  [55.81687911]\n",
      "Iteration Number:  5  Iteration Reward:  -13.067688342325768 Final F Value:  [66.94389829]\n",
      "Weights Updated\n",
      "Iteration Number:  6  Iteration Reward:  -11.539773668500988 Final F Value:  [62.24021015]\n",
      "Iteration Number:  7  Iteration Reward:  -9.758268292712964 Final F Value:  [45.62268548]\n",
      "Iteration Number:  8  Iteration Reward:  -13.093673888213806 Final F Value:  [92.42444889]\n",
      "Iteration Number:  9  Iteration Reward:  -8.856676037286265 Final F Value:  [46.12029396]\n",
      "Iteration Number:  10  Iteration Reward:  -10.109841314480821 Final F Value:  [60.17543889]\n",
      "Weights Updated\n",
      "Iteration Number:  11  Iteration Reward:  -13.857969027064264 Final F Value:  [82.91414654]\n",
      "Iteration Number:  12  Iteration Reward:  -11.515494681807207 Final F Value:  [77.87210451]\n",
      "Iteration Number:  13  Iteration Reward:  -9.80853251103354 Final F Value:  [51.74813184]\n",
      "Iteration Number:  14  Iteration Reward:  -11.261001983691107 Final F Value:  [70.08295535]\n",
      "Iteration Number:  15  Iteration Reward:  -11.48158627399732 Final F Value:  [65.85329049]\n",
      "Weights Updated\n",
      "Iteration Number:  16  Iteration Reward:  -11.49308327074904 Final F Value:  [62.11961359]\n",
      "Iteration Number:  17  Iteration Reward:  -12.268907468942906 Final F Value:  [67.9941226]\n",
      "Iteration Number:  18  Iteration Reward:  -12.648542098361002 Final F Value:  [60.42780745]\n",
      "Iteration Number:  19  Iteration Reward:  -10.64247117888589 Final F Value:  [45.55060872]\n",
      "Iteration Number:  20  Iteration Reward:  -12.043272207948137 Final F Value:  [75.21695463]\n",
      "Weights Updated\n",
      "Iteration Number:  21  Iteration Reward:  -5.965443437716064 Final F Value:  [22.85055732]\n",
      "Iteration Number:  22  Iteration Reward:  -9.98177887967795 Final F Value:  [74.29156396]\n",
      "Iteration Number:  23  Iteration Reward:  -9.270689909623501 Final F Value:  [36.18277686]\n",
      "Iteration Number:  24  Iteration Reward:  -7.334747135811338 Final F Value:  [39.24607319]\n",
      "Iteration Number:  25  Iteration Reward:  -11.698894415384045 Final F Value:  [75.14065431]\n"
     ]
    }
   ],
   "source": [
    "obs_dim=4\n",
    "act_dim=3\n",
    "gamma=0.99\n",
    "lamda=0.10\n",
    "entropy_coef=0.001\n",
    "epsilon=0.2\n",
    "value_range=0.2\n",
    "num_epochs=10\n",
    "batch_size=40\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-4\n",
    "\n",
    "ppo_agent=PPO_Agent(obs_dim,act_dim,gamma,lamda,entropy_coef,epsilon,value_range,\n",
    "                    num_epochs,batch_size,actor_lr,critic_lr)\n",
    "\n",
    "env=Environment_Rastingin()\n",
    "\n",
    "shedding_freq=8.64\n",
    "dur_actions=0.25\n",
    "CFD_timestep=5e-4\n",
    "mot_timestep=5e-4\n",
    "ramp_time=0.05\n",
    "num_actions=20\n",
    "num_policies=60\n",
    "num_iterations=5\n",
    "is_evaluate=False\n",
    "total_rewards=[]\n",
    "\n",
    "for i in range(num_policies):\n",
    "    Iterations=[]\n",
    "    for j in range(num_iterations):\n",
    "        iteration_ID=num_iterations*(i)+j+1\n",
    "        Iterations.append(Iteration(iteration_ID,shedding_freq,num_actions,dur_actions,CFD_timestep,mot_timestep,\n",
    "                                   ramp_time))\n",
    "    for j in range(num_iterations):\n",
    "        Iterations[j].run_iteration()\n",
    "    #for j in range(num_iterations):\n",
    "    #    Iterations[j].save_iteration()\n",
    "    for j in range(num_iterations):\n",
    "        ppo_agent.states.extend(Iterations[j].states)\n",
    "        ppo_agent.actions.extend(Iterations[j].actions)\n",
    "        ppo_agent.rewards.extend(Iterations[j].rewards)\n",
    "        ppo_agent.is_terminals.extend(Iterations[j].is_terminals)\n",
    "        ppo_agent.log_probs.extend(Iterations[j].log_probs)\n",
    "        ppo_agent.values.extend(Iterations[j].values)\n",
    "        ppo_agent.next_states.extend(Iterations[j].next_states)\n",
    "        total_reward=np.sum(Iterations[j].rewards)\n",
    "        final_state=Iterations[j].next_states[-1]\n",
    "        final_f_value=env.calculate_final_f(final_state)\n",
    "        print('Iteration Number: ',Iterations[j].iteration_ID,' Iteration Reward: ',total_reward,\n",
    "             'Final F Value: ', final_f_value)\n",
    "        total_rewards.append(total_reward)\n",
    "    \n",
    "    next_state=ppo_agent.next_states[-1]\n",
    "    value = ppo_agent.critic.forward(torch.FloatTensor(next_state))\n",
    "    value=value.detach().numpy()\n",
    "    ppo_agent.values.append(value)\n",
    "    ppo_agent._update_weights()\n",
    "    #ppo_agent._save_weights(i)\n",
    "    print('Weights Updated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "final_rewards=25*np.array(total_rewards)\n",
    "plt.plot(final_rewards)\n",
    "plt.rcParams['xtick.labelsize']=25\n",
    "plt.rcParams['ytick.labelsize']=25\n",
    "plt.xlabel('Episode Number',fontsize=20)\n",
    "plt.ylabel('Total Episode Reward',fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
