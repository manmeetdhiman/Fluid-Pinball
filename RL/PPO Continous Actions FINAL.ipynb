{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actor network class created below\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int,):\n",
    "        \n",
    "        self.in_dim=in_dim\n",
    "        self.out_dim=out_dim\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.hidden_one = nn.Linear(in_dim, 150)\n",
    "        self.hidden_two = nn.Linear(150,150)\n",
    "        self.mu_layer = nn.Linear(150, out_dim)\n",
    "        #self.log_std_layer = nn.Linear(150, out_dim)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        x = F.tanh(self.hidden_one(state))\n",
    "        x = F.tanh(self.hidden_two(x))\n",
    "        \n",
    "        mu = torch.tanh(self.mu_layer(x))\n",
    "        #log_std = torch.tanh(self.log_std_layer(x))\n",
    "\n",
    "        #std = torch.exp(log_std)\n",
    "        std = torch.ones((1,self.out_dim), dtype=torch.float64)\n",
    "        std = std.new_full((1, self.out_dim), 0.5)\n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.sample()\n",
    "\n",
    "        return action, dist\n",
    "    \n",
    "#Critic network class created below    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.hidden_one = nn.Linear(in_dim, 120)\n",
    "        self.hidden_two = nn.Linear(120,120)\n",
    "        self.out = nn.Linear(120, 1)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        x = F.relu(self.hidden_one(state))\n",
    "        x = F.relu(self.hidden_two(x))\n",
    "        value = self.out(x)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function calculates the gae based on immediate reward and state values \n",
    "\n",
    "def get_gae(rewards: list, values: list, is_terminals: list, gamma: float, lamda: float,):\n",
    "    \n",
    "    gae = 0\n",
    "    returns = []\n",
    "    \n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = (rewards[i] + gamma * values[i + 1] * is_terminals[i] - values[i])\n",
    "        gae = delta + gamma * lamda * is_terminals[i] * gae\n",
    "        returns.insert(0, gae + values[i])\n",
    "\n",
    "    return returns\n",
    "\n",
    "\n",
    "#Function looks at all of the states, actions, returns, etc. and makes training batches\n",
    "\n",
    "def trajectories_data_generator(states: torch.Tensor,actions: torch.Tensor,\n",
    "                                returns: torch.Tensor,log_probs: torch.Tensor,\n",
    "                                values: torch.Tensor,advantages: torch.Tensor,\n",
    "                                batch_size, num_epochs,):\n",
    "\n",
    "    data_len = states.size(0)\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(data_len // batch_size):\n",
    "            ids = np.random.choice(data_len, batch_size)\n",
    "            yield states[ids, :], actions[ids], returns[ids], log_probs[ids], values[ids], advantages[ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, x=50.0,y=50.0,z=50.0):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.z=z\n",
    "        self.x_reset=x\n",
    "        self.y_reset=y\n",
    "        self.z_reset=z\n",
    "        self.f=self.calculate_f()\n",
    "        self.counter=0\n",
    "        self.done=False\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.x=self.x_reset\n",
    "        self.y=self.y_reset\n",
    "        self.z=self.z_reset\n",
    "        self.f=self.calculate_f()\n",
    "        self.counter=0\n",
    "        self.done=False\n",
    "        observation=np.array([self.x/100.0,self.y/100.,self.z/100.0])\n",
    "        return(observation)\n",
    "    \n",
    "    def step (self,action):\n",
    "        action_x=action[0][0]*5.0\n",
    "        action_y=action[0][1]*5.0\n",
    "        action_z=action[0][2]*5.0\n",
    "        \n",
    "        self.x+=action_x\n",
    "        self.y+=action_y\n",
    "        self.z+=action_z\n",
    "                \n",
    "        f_=self.calculate_f()\n",
    "        reward= np.float64((self.f-f_)/150)\n",
    "        self.f=f_\n",
    "        self.counter +=1\n",
    "        if self.counter>=150:\n",
    "            self.done=True\n",
    "        observation=np.array([self.x/100.0,self.y/100.0,self.z/100.0])\n",
    "        \n",
    "        return observation,reward,self.done\n",
    "    \n",
    "    def calculate_f(self):\n",
    "        f=(self.x-4.0)**2.0+(self.y-8.0)**2.0+(self.z-9.0)**2.0\n",
    "        return f\n",
    "    \n",
    "    def return_f_value(self,state):\n",
    "        x=state[0][0]*100\n",
    "        y=state[0][1]*100\n",
    "        z=state[0][2]*100\n",
    "        f=np.array([(x-4.0)**2.0+(y-8.0)**2.0+(z-9.0)**2])\n",
    "        return f\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(object):\n",
    "    def __init__(self, obs_dim=3, act_dim =3, gamma =0.99,lamda =0.9,\n",
    "                 entropy_coef =0.01,epsilon =0.2, value_range =0.2,\n",
    "                 rollout_len =150, total_rollouts =300, num_epochs =10,\n",
    "                 batch_size =30,is_evaluate =False, solved_reward = None,\n",
    "                 actor_lr =0.001, critic_lr =0.001):\n",
    "        \n",
    "\n",
    "        \n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.env = Environment()\n",
    "        \n",
    "        self.gamma=gamma\n",
    "        self.lamda=lamda\n",
    "        self.entropy_coef=entropy_coef\n",
    "        self.epsilon=epsilon\n",
    "        self.value_range=value_range\n",
    "        \n",
    "        self.rollout_len=rollout_len\n",
    "        self.total_rollouts=total_rollouts\n",
    "        self.num_epochs=num_epochs\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.obs_dim=obs_dim\n",
    "        self.act_dim=act_dim\n",
    "        self.actor_lr=actor_lr\n",
    "        self.critic_lr=critic_lr\n",
    "        \n",
    "        self.actor=Actor(self.obs_dim,self.act_dim)\n",
    "        self.critic=Critic(self.obs_dim)\n",
    "        self.actor_optimizer=optim.Adam(self.actor.parameters(),lr=self.actor_lr)\n",
    "        self.critic_optimizer=optim.Adam(self.critic.parameters(),lr=self.critic_lr)\n",
    "            \n",
    "        # Memory for the trajectory\n",
    "        self.memory=Memory()\n",
    "            \n",
    "        # Memory of the train history to see how agent is improving\n",
    "        self.actor_loss_history=[]\n",
    "        self.critic_loss_history=[]\n",
    "        self.scores=[]\n",
    "            \n",
    "        self.is_evaluate=is_evaluate\n",
    "        self.solved_reward=solved_reward\n",
    "        \n",
    "    def _get_action(self,state):\n",
    "        \n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action,dist=self.actor.forward(state)\n",
    "        #print('action',action)\n",
    "            \n",
    "        if not self.is_evaluate:\n",
    "            value=self.critic.forward(state)\n",
    "            #print('value',value)\n",
    "                \n",
    "        #Store trajectory in memory class\n",
    "        self.memory.states.append(state)\n",
    "        self.memory.actions.append(action)\n",
    "        self.memory.log_probs.append(dist.log_prob(action))\n",
    "        self.memory.values.append(value)\n",
    "        #print('self memory states',self.memory.states)\n",
    "        #print('self memory actions', self.memory.actions)\n",
    "        #print('self memory log probs', self.memory.log_probs)\n",
    "        #print('self memory values', self.memory.values)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def _step(self, action):\n",
    "        next_state, reward, done = self.env.step(action)\n",
    "        #print('next_state',next_state,next_state.shape)\n",
    "        #print('reward', reward, reward.shape)\n",
    "        #print('done',done)\n",
    "\n",
    "\n",
    "        # add fake dim to match dimension with batch size\n",
    "        next_state = np.reshape(next_state, (1, -1)).astype(np.float64)\n",
    "        reward = np.reshape(reward, (1, -1)).astype(np.float64)\n",
    "        done = np.reshape(done, (1, -1))\n",
    "\n",
    "        if not self.is_evaluate:\n",
    "            self.memory.rewards.append(torch.FloatTensor(reward).to(self.device))\n",
    "            self.memory.is_terminals.append(torch.FloatTensor(1 - done).to(self.device))\n",
    "        \n",
    "        #print('self.memory.rewards',self.memory.rewards)\n",
    "        #print('self.memory.is_terminal', self.memory.is_terminals)\n",
    "\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    \n",
    "    def _update_weights(self):\n",
    "        \n",
    "        returns = get_gae(self.memory.rewards, self.memory.values,self.memory.is_terminals,\n",
    "                          self.gamma,self.lamda,)\n",
    "\n",
    "        # flattening a list of torch.tensors into vectors\n",
    "        states = torch.cat(self.memory.states).view(-1, self.obs_dim)\n",
    "        actions = torch.cat(self.memory.actions)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        log_probs = torch.cat(self.memory.log_probs).detach()\n",
    "        values = torch.cat(self.memory.values).detach()\n",
    "        advantages = returns - values[:-1]\n",
    "\n",
    "        for state, action, return_, old_log_prob, old_value, advantage in trajectories_data_generator(\n",
    "            states=states,actions=actions, returns=returns,log_probs=log_probs,values=values,advantages=advantages,\n",
    "            batch_size=self.batch_size,num_epochs=self.num_epochs,):\n",
    "\n",
    "            # compute ratio (pi_theta / pi_theta__old)\n",
    "            _, dist = self.actor(state)\n",
    "            cur_log_prob = dist.log_prob(action)\n",
    "            ratio = torch.exp(cur_log_prob - old_log_prob)\n",
    "\n",
    "            # compute entropy\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            # compute actor loss\n",
    "            loss =  advantage * ratio\n",
    "            clipped_loss = (torch.clamp(ratio, 1. - self.epsilon, 1. + self.epsilon)\n",
    "                            * advantage)\n",
    "            actor_loss = (-torch.mean(torch.min(loss, clipped_loss))\n",
    "                          - entropy * self.entropy_coef)\n",
    "            \n",
    "            # critic loss, uncoment for clipped value loss too.\n",
    "            cur_value = self.critic(state)\n",
    "            #clipped_value = (\n",
    "            #    old_value + torch.clamp(cur_value - old_value,\n",
    "            #                            -self.value_range, self.value_range)\n",
    "            #   )\n",
    "            #loss = (return_ - cur_value).pow(2)\n",
    "            #clipped_loss = (return_ - clipped_value).pow(2)\n",
    "            #critic_loss = torch.mean(torch.max(loss, clipped_loss))\n",
    "\n",
    "            critic_loss = (return_ - cur_value).pow(2).mean()\n",
    "\n",
    "            # actor optimizer step\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # critic optimizer step\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "            \n",
    "            self.memory.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim=3\n",
    "act_dim=3\n",
    "gamma=0.95\n",
    "lamda=0.95\n",
    "entropy_coef=0.005\n",
    "epsilon=0.2\n",
    "value_range=0.2\n",
    "rollout_len=150\n",
    "total_rollouts=500\n",
    "num_epochs=15\n",
    "batch_size=50\n",
    "is_evaluate=False\n",
    "solved_reward = None\n",
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-3\n",
    "\n",
    "ppo_agent=PPOAgent(obs_dim,act_dim,gamma,lamda,entropy_coef,epsilon,value_range,\n",
    "                   rollout_len,total_rollouts,num_epochs,batch_size,is_evaluate,\n",
    "                   solved_reward,actor_lr,critic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "score = 0\n",
    "state = ppo_agent.env.reset()\n",
    "state = np.reshape(state, (1, -1))\n",
    "ending_states=[]\n",
    "\n",
    "for rollout in range(ppo_agent.total_rollouts):\n",
    "    for step in range(ppo_agent.rollout_len):\n",
    "        action = ppo_agent._get_action(state)\n",
    "        next_state, reward, done = ppo_agent._step(action)\n",
    "        state = next_state\n",
    "        score += reward[0][0]\n",
    "        \n",
    "        if done[0][0]:\n",
    "            ppo_agent.scores.append(score)\n",
    "            ending_state=ppo_agent.env.return_f_value(state)\n",
    "            print('Rollout Number: ', rollout, ' Total Rewards: ', np.round(score,1), ' x:',\n",
    "                  np.round(state[0][0]*100,1) ,' y: ', np.round(state[0][1]*100,1),' z: ', \n",
    "                  np.round(state[0][2]*100,1) , ' f: ', np.round(ending_state,1))\n",
    "            ending_states.append(ending_state)\n",
    "            score = 0\n",
    "            state = ppo_agent.env.reset()\n",
    "            state = np.reshape(state, (1, -1))\n",
    "\n",
    "    if ppo_agent.solved_reward is not None:\n",
    "        if np.mean(ppo_agent.scores[-10:]) > self.solved_reward:\n",
    "            print(\"Congratulations, it's solved!\")\n",
    "            break\n",
    "\n",
    "    value = ppo_agent.critic.forward(torch.FloatTensor(next_state))\n",
    "    ppo_agent.memory.values.append(value)\n",
    "    ppo_agent._update_weights()\n",
    "\n",
    "#print(ppo_agent.scores)\n",
    "#print(ending_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
